services:
  api:
    build:
      context: .
    container_name: ai-backend-api
    ports:
      - "8080:8080"
    environment:
      OTEL_SERVICE_NAME: ai-backend-api
      OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc
      LOG_LEVEL: info
      LLM_BASE_URL: http://llm:11434
    depends_on:
      - llm
      - tempo
    networks:
      - observability

  llm:
    image: ollama/ollama:latest
    container_name: llm
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - observability

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
    networks:
      - observability

  loki:
    image: grafana/loki:latest
    container_name: loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - observability

  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./tempo/tempo.yaml:/etc/tempo.yaml
    ports:
      - "4317:4317"
    networks:
      - observability

  alloy:
    image: grafana/alloy:latest
    container_name: alloy
    volumes:
      - ./alloy:/etc/alloy
      - ./alloy/config.alloy:/etc/alloy/config.alloy:ro
    command: run /etc/alloy/config.alloy
    depends_on:
      - loki
    networks:
      - observability

volumes:
  grafana-data:
  ollama-data:


networks:
  observability:
    driver: bridge
