services:
  api:
    build:
      context: .
    container_name: ai-backend-api
    ports:
      - "8080:8080"
    environment:
      OTEL_SERVICE_NAME: ai-backend-api
      OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo:4317
      OTEL_EXPORTER_OTLP_PROTOCOL: grpc
      LOG_LEVEL: info
      OLLAMA_URL: http://ollama:11434
    logging:
      driver: "json-file"
    depends_on:
      - ollama
      - tempo
    networks:
      - observability

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - observability

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
    networks:
      - observability
    depends_on:
      - loki
      - tempo

  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./tempo/tempo.yaml:/etc/tempo.yaml
    ports:
      - "4317:4317"
      - "4318:4318"
      - "3200:3200"
    networks:
      - observability

  loki:
    image: grafana/loki:latest
    container_name: loki
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./loki-config.yaml:/etc/loki/local-config.yaml
    ports:
      - "3100:3100"
    networks:
      - observability

  alloy:
    image: grafana/alloy:latest
    container_name: alloy
    volumes:
      - ./alloy-config.alloy:/etc/alloy/config.alloy:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: run /etc/alloy/config.alloy
    depends_on:
      - loki
    networks:
      - observability

volumes:
  grafana-storage:
  ollama-data:


networks:
  observability:
    driver: bridge
